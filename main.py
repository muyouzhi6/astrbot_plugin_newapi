import asyncio
import json
import time
from datetime import datetime, timezone, timedelta
from pathlib import Path
from typing import Any, Dict, List, Tuple
from urllib.parse import urlencode

import astrbot.api.message_components as Comp
from astrbot.api import AstrBotConfig, logger
from astrbot.api.event import AstrMessageEvent, filter
from astrbot.api.star import Context, Star, register


@register(
    "newapi",
    "æœ¨æœ‰çŸ¥",
    "NewAPI è¿ç»´åŠ©æ‰‹ï¼šæ¦‚è§ˆ/æ¨¡å‹/æ—¥å¿—/é¢åº¦/å¼‚å¸¸/åˆ†æ/å»ºè®®/å¥åº·ï¼ˆä¸­æ–‡ç®€æŒ‡ä»¤ï¼‰",
    "2.3.1",
)
class NewAPIPlugin(Star):
    def __init__(self, context: Context, config: AstrBotConfig):
        super().__init__(context)
        self.config = config

        self.base_domain: str = str(config.get("base_domain", "")).strip().rstrip("/")
        self.authorization: str = str(config.get("authorization", "")).strip()
        self.new_api_user: str = str(config.get("new_api_user", "")).strip()
        self.request_timeout: int = int(config.get("request_timeout", 15) or 15)

        self.default_window_hours: int = int(config.get("default_window_hours", 24) or 24)
        self.default_top_n: int = int(config.get("default_top_n", 5) or 5)
        self.default_log_limit: int = int(config.get("log_page_size", 20) or 20)

        self.use_forward: bool = bool(config.get("use_forward", True))
        self.log_use_forward: bool = bool(config.get("log_use_forward", self.use_forward))
        self.user_use_forward: bool = bool(config.get("user_use_forward", False))

        self.llm_enabled: bool = bool(config.get("llm_enabled", False))
        self.llm_use_current_provider: bool = bool(config.get("llm_use_current_provider", True))
        self.llm_provider_id: str = str(config.get("llm_provider_id", "")).strip()

        self._setup_data_paths()

    def _setup_data_paths(self):
        plugin_name = "newapi"
        self.plugin_data_dir = Path(__file__).resolve().parent / "data"
        try:
            from astrbot.core.utils.astrbot_path import get_astrbot_data_path  # type: ignore

            self.plugin_data_dir = get_astrbot_data_path() / "plugin_data" / plugin_name
        except Exception:
            pass
        self.plugin_data_dir.mkdir(parents=True, exist_ok=True)
        self.cache_file = self.plugin_data_dir / "last_usage_payload.json"

    def _headers(self) -> Dict[str, str]:
        h = {"Accept": "application/json"}
        if self.authorization:
            h["Authorization"] = self.authorization
        if self.new_api_user:
            h["New-Api-User"] = self.new_api_user
        return h

    async def _http_get_json(self, url: str, headers: Dict[str, str]) -> Any:
        from urllib.error import HTTPError, URLError
        from urllib.request import Request, urlopen

        def _do() -> Any:
            req = Request(url=url, method="GET")
            for k, v in headers.items():
                req.add_header(k, v)
            with urlopen(req, timeout=self.request_timeout) as resp:
                body = resp.read().decode("utf-8", errors="ignore")
                return json.loads(body)

        try:
            return await asyncio.to_thread(_do)
        except HTTPError as e:
            return {"error": f"HTTP {e.code}"}
        except URLError as e:
            return {"error": f"URL {e.reason}"}
        except Exception as e:
            return {"error": str(e)}

    async def _http_post_json(self, url: str, headers: Dict[str, str], payload: Dict[str, Any], timeout_sec: int) -> Any:
        from urllib.error import HTTPError, URLError
        from urllib.request import Request, urlopen

        def _do() -> Any:
            req = Request(url=url, method="POST")
            for k, v in headers.items():
                req.add_header(k, v)
            data = json.dumps(payload, ensure_ascii=False).encode("utf-8")
            with urlopen(req, data=data, timeout=timeout_sec) as resp:
                body = resp.read().decode("utf-8", errors="ignore")
                return json.loads(body)

        try:
            return await asyncio.to_thread(_do)
        except HTTPError as e:
            return {"error": f"HTTP {e.code}"}
        except URLError as e:
            return {"error": f"URL {e.reason}"}
        except Exception as e:
            return {"error": str(e)}

    def _extract_records(self, payload: Any) -> List[Dict[str, Any]]:
        if isinstance(payload, list):
            return payload
        if not isinstance(payload, dict):
            return []
        if payload.get("success") is False and payload.get("message"):
            logger.warning(f"newapi usage api failed: {payload.get('message')}")
            return []
        for key in ("data", "list", "items"):
            v = payload.get(key)
            if isinstance(v, list):
                return v
            if isinstance(v, dict):
                for k2 in ("data", "list", "items"):
                    vv = v.get(k2)
                    if isinstance(vv, list):
                        return vv
        return []

    def _fmt_ts(self, ts: int) -> str:
        tz = timezone(timedelta(hours=8), name="CST+8")
        return datetime.fromtimestamp(ts, tz).strftime("%m-%d %H:%M")

    def _window(self, hours: int) -> Tuple[int, int]:
        end_ts = int(time.time())
        start_ts = end_ts - int(hours) * 3600
        return start_ts, end_ts

    async def _fetch_usage_payload(self, hours: int) -> Any:
        if not self.base_domain:
            return {"error": "missing base_domain"}
        start_ts, end_ts = self._window(hours)
        q = urlencode(
            {
                "username": "",
                "start_timestamp": str(start_ts),
                "end_timestamp": str(end_ts),
                "default_time": "hour",
            }
        )
        url = f"{self.base_domain}/api/data/self?{q}"
        payload = await self._http_get_json(url, self._headers())
        if isinstance(payload, (dict, list)) and not (isinstance(payload, dict) and payload.get("error")):
            try:
                self.cache_file.write_text(json.dumps(payload, ensure_ascii=False), encoding="utf-8")
            except Exception:
                pass
        return payload

    async def _fetch_logs_payload(self, limit: int, hours: int = 24) -> Any:
        if not self.base_domain:
            return {"error": "missing base_domain"}
        start_ts, end_ts = self._window(hours)
        q = urlencode(
            {
                "p": 1,
                "page_size": max(1, min(limit, 100)),
                "type": 0,
                "start_timestamp": str(start_ts),
                "end_timestamp": str(end_ts),
            }
        )
        url = f"{self.base_domain}/api/log/?{q}"
        return await self._http_get_json(url, self._headers())

    async def _fetch_user_self(self) -> Any:
        if not self.base_domain:
            return {"error": "missing base_domain"}
        return await self._http_get_json(f"{self.base_domain}/api/user/self", self._headers())

    def _aggregate(self, records: List[Dict[str, Any]], start_ts: int, end_ts: int) -> Tuple[Dict[str, Any], List[Tuple[str, Dict[str, int]]]]:
        total_tokens = 0
        total_requests = 0
        total_quota = 0
        model_stats: Dict[str, Dict[str, int]] = {}

        for r in records:
            ts = int(r.get("created_at", 0) or 0)
            if ts < start_ts or ts > end_ts:
                continue
            model = str(r.get("model_name") or "æœªçŸ¥æ¨¡å‹")
            token = int(r.get("token_used", 0) or 0)
            cnt = int(r.get("count", 0) or 0)
            quota = int(r.get("quota", 0) or 0)

            total_tokens += token
            total_requests += cnt
            total_quota += quota

            s = model_stats.setdefault(model, {"token": 0, "count": 0, "quota": 0})
            s["token"] += token
            s["count"] += cnt
            s["quota"] += quota

        minutes = max(1, int((end_ts - start_ts) / 60))
        stats = {
            "start_ts": start_ts,
            "end_ts": end_ts,
            "tokens": total_tokens,
            "requests": total_requests,
            "quota": total_quota,
            "rpm": total_requests / minutes,
            "tpm": total_tokens / minutes,
            "minutes": minutes,
        }
        sorted_models = sorted(model_stats.items(), key=lambda kv: kv[1]["count"], reverse=True)
        return stats, sorted_models

    def _aggregate_by_keys(
        self,
        records: List[Dict[str, Any]],
        start_ts: int,
        end_ts: int,
        key_candidates: List[str],
        fallback: str = "æœªçŸ¥",
    ) -> List[Tuple[str, Dict[str, int]]]:
        stats: Dict[str, Dict[str, int]] = {}
        for r in records:
            ts = int(r.get("created_at", 0) or 0)
            if ts < start_ts or ts > end_ts:
                continue

            k = fallback
            for key in key_candidates:
                v = r.get(key)
                if v not in (None, "", 0):
                    k = str(v)
                    break

            token = int(r.get("token_used", 0) or 0)
            cnt = int(r.get("count", 0) or 0)
            quota = int(r.get("quota", 0) or 0)
            s = stats.setdefault(k, {"token": 0, "count": 0, "quota": 0})
            s["token"] += token
            s["count"] += cnt
            s["quota"] += quota

        return sorted(stats.items(), key=lambda kv: kv[1]["token"], reverse=True)

    def _percentile(self, nums: List[int], q: float) -> int:
        if not nums:
            return 0
        arr = sorted(nums)
        idx = min(len(arr) - 1, max(0, int((len(arr) - 1) * q)))
        return int(arr[idx])

    def _summarize_log_metrics(self, items: List[Dict[str, Any]]) -> Dict[str, Any]:
        total = len(items)
        lat = [int(it.get("use_time", 0) or 0) for it in items]
        err = [it for it in items if int(it.get("type", 0) or 0) == 5 or int(it.get("code", 0) or 0) >= 400]
        slow = [it for it in items if int(it.get("use_time", 0) or 0) >= 15000]

        code_dist: Dict[str, int] = {}
        model_dist: Dict[str, int] = {}
        chan_dist: Dict[str, int] = {}
        for it in items:
            c = str(int(it.get("code", 0) or 0))
            code_dist[c] = code_dist.get(c, 0) + 1

            m = str(it.get("model_name") or "æœªçŸ¥æ¨¡å‹")
            model_dist[m] = model_dist.get(m, 0) + 1

            chan = str(
                it.get("channel_name")
                or it.get("channel")
                or it.get("channel_id")
                or it.get("provider_name")
                or it.get("provider")
                or it.get("provider_id")
                or "æœªçŸ¥æ¸ é“"
            )
            chan_dist[chan] = chan_dist.get(chan, 0) + 1

        return {
            "total": total,
            "err_count": len(err),
            "err_rate": len(err) / max(1, total),
            "slow15_count": len(slow),
            "slow15_rate": len(slow) / max(1, total),
            "avg_ms": int(sum(lat) / max(1, total)) if lat else 0,
            "p50_ms": self._percentile(lat, 0.5),
            "p95_ms": self._percentile(lat, 0.95),
            "p99_ms": self._percentile(lat, 0.99),
            "code_top": sorted(code_dist.items(), key=lambda kv: kv[1], reverse=True)[:6],
            "model_top": sorted(model_dist.items(), key=lambda kv: kv[1], reverse=True)[:5],
            "channel_top": sorted(chan_dist.items(), key=lambda kv: kv[1], reverse=True)[:5],
            "err_items": err,
            "slow_items": sorted(items, key=lambda x: int(x.get("use_time", 0) or 0), reverse=True)[:8],
        }

    def _format_overview(self, stats: Dict[str, Any], top_models: List[Tuple[str, Dict[str, int]]], top_n: int) -> str:
        lines = [
            "ğŸ“Š NewAPI æ¦‚è§ˆ",
            f"æ—¶é—´: {self._fmt_ts(stats['start_ts'])} - {self._fmt_ts(stats['end_ts'])}",
            f"æ€» tokens: {stats['tokens']:,}",
            f"æ€»è¯·æ±‚: {stats['requests']:,}",
            f"æ€»é…é¢: {stats['quota']:,}",
            f"å¹³å‡ RPM: {stats['rpm']:.3f}",
            f"å¹³å‡ TPM: {stats['tpm']:.3f}",
        ]
        if top_models and top_n > 0:
            lines.append(f"\nğŸ”¥ Top{top_n} æ¨¡å‹:")
            total_token = max(1, int(stats.get("tokens", 0) or 0))
            for i, (m, s) in enumerate(top_models[:top_n], 1):
                pct = (int(s['token']) / total_token) * 100
                lines.append(f"{i}. {m} | è¯·æ±‚{s['count']:,} | token{s['token']:,} ({pct:.1f}%)")
        return "\n".join(lines)

    def _format_dual_window_report(
        self,
        stats_24: Dict[str, Any],
        models_24: List[Tuple[str, Dict[str, int]]],
        channels_24: List[Tuple[str, Dict[str, int]]],
        stats_2: Dict[str, Any],
        models_2: List[Tuple[str, Dict[str, int]]],
        channels_2: List[Tuple[str, Dict[str, int]]],
        log_chan_24: List[Tuple[str, int]],
        log_chan_2: List[Tuple[str, int]],
    ) -> str:
        def _line(name: str, s: Dict[str, int], total_token: int) -> str:
            pct = (int(s.get("token", 0)) / max(1, total_token)) * 100
            return f"{name} | token {int(s.get('token',0)):,} ({pct:.1f}%) | req {int(s.get('count',0)):,}"

        out = ["ğŸ“ˆ æ¶ˆè€—å¯¹æ¯”ï¼ˆ24h vs 2hï¼‰"]
        out.append(
            f"24h: token {stats_24['tokens']:,} | req {stats_24['requests']:,} | quota {stats_24['quota']:,} | RPM {stats_24['rpm']:.2f}"
        )
        out.append(
            f"2h : token {stats_2['tokens']:,} | req {stats_2['requests']:,} | quota {stats_2['quota']:,} | RPM {stats_2['rpm']:.2f}"
        )

        out.append("\nğŸ¤– 24h æ¨¡å‹é›†ä¸­åº¦")
        for m, s in models_24[:5]:
            out.append("- " + _line(m, s, int(stats_24['tokens'])))

        out.append("\nğŸ¤– 2h æ¨¡å‹é›†ä¸­åº¦")
        if models_2:
            for m, s in models_2[:5]:
                out.append("- " + _line(m, s, int(stats_2['tokens'])))
        else:
            out.append("- æš‚æ— æ•°æ®")

        usage_chan_24_valid = any(c != "æœªçŸ¥æ¸ é“" for c, _ in channels_24)
        usage_chan_2_valid = any(c != "æœªçŸ¥æ¸ é“" for c, _ in channels_2)

        out.append("\nğŸ›£ï¸ 24h æ¸ é“é›†ä¸­åº¦")
        if usage_chan_24_valid:
            for c, s in channels_24[:5]:
                out.append("- " + _line(c, s, int(stats_24['tokens'])))
        elif log_chan_24:
            total_req_24 = max(1, sum(n for _, n in log_chan_24))
            out.append("- usage æ¥å£ç¼ºå°‘æ¸ é“å­—æ®µï¼Œä»¥ä¸‹åŸºäºæ—¥å¿—è¯·æ±‚æ•°")
            for c, n in log_chan_24[:5]:
                out.append(f"- {c} | req {n:,} ({n/total_req_24:.1%})")
        else:
            out.append("- æš‚æ— æ¸ é“æ•°æ®")

        out.append("\nğŸ›£ï¸ 2h æ¸ é“é›†ä¸­åº¦")
        if usage_chan_2_valid:
            for c, s in channels_2[:5]:
                out.append("- " + _line(c, s, int(stats_2['tokens'])))
        elif log_chan_2:
            total_req_2 = max(1, sum(n for _, n in log_chan_2))
            out.append("- usage æ¥å£ç¼ºå°‘æ¸ é“å­—æ®µï¼Œä»¥ä¸‹åŸºäºæ—¥å¿—è¯·æ±‚æ•°")
            for c, n in log_chan_2[:5]:
                out.append(f"- {c} | req {n:,} ({n/total_req_2:.1%})")
        else:
            out.append("- æš‚æ— æ¸ é“æ•°æ®")

        if stats_24['tokens'] > 0:
            ratio = (stats_2['tokens'] / stats_24['tokens']) * 100
            out.append(f"\nğŸ” è¿‘2h tokenå 24hæ¯”ä¾‹: {ratio:.1f}%")

        return "\n".join(out)

    def _extract_log_items(self, payload: Any) -> List[Dict[str, Any]]:
        if isinstance(payload, list):
            return payload
        if not isinstance(payload, dict):
            return []
        if payload.get("success") is False and payload.get("message"):
            logger.warning(f"newapi log api failed: {payload.get('message')}")
            return []
        d = payload.get("data")
        if isinstance(d, dict):
            for k in ("items", "list", "data"):
                v = d.get(k)
                if isinstance(v, list):
                    return v
        for k in ("items", "list"):
            v = payload.get(k)
            if isinstance(v, list):
                return v
        return []

    def _format_logs(self, items: List[Dict[str, Any]]) -> str:
        if not items:
            return "ğŸ“œ è°ƒç”¨æ—¥å¿—\næš‚æ— æ•°æ®ï¼ˆå¯èƒ½æ˜¯æ—¶é—´çª—å£å†…æ— è¯·æ±‚æˆ–æ¥å£æœªè¿”å›æ˜ç»†ï¼‰"

        m = self._summarize_log_metrics(items)

        out = ["ğŸ“œ è°ƒç”¨æ—¥å¿—æ€»è§ˆ"]
        out.append(
            f"æ€»è¯·æ±‚ {m['total']} | é”™è¯¯ {m['err_count']} ({m['err_rate']:.1%}) | è¶…æ…¢>=15s {m['slow15_count']} ({m['slow15_rate']:.1%})"
        )
        out.append(
            f"è€—æ—¶: avg {m['avg_ms']}ms | p50 {m['p50_ms']}ms | p95 {m['p95_ms']}ms | p99 {m['p99_ms']}ms"
        )

        if m["code_top"]:
            out.append("çŠ¶æ€ç åˆ†å¸ƒ: " + "ï¼Œ".join([f"{c}({n})" for c, n in m["code_top"]]))
        if m["model_top"]:
            out.append("ä¸»åŠ›æ¨¡å‹: " + "ï¼Œ".join([f"{x}({n})" for x, n in m["model_top"][:3]]))
        if m["channel_top"]:
            out.append("ä¸»åŠ›æ¸ é“: " + "ï¼Œ".join([f"{x}({n})" for x, n in m["channel_top"][:3]]))

        out.append("\nğŸ¢ æœ€æ…¢è¯·æ±‚ Top5")
        for it in m["slow_items"][:5]:
            t = int(it.get("created_at", 0) or 0)
            mod = str(it.get("model_name") or "æœªçŸ¥æ¨¡å‹")
            code = int(it.get("code", 0) or 0)
            use = int(it.get("use_time", 0) or 0)
            pt = int(it.get("prompt_tokens", 0) or 0)
            ct = int(it.get("completion_tokens", 0) or 0)
            out.append(f"- {self._fmt_ts(t)} | {mod} | code={code} | {use}ms | token {pt}/{ct}")

        out.append("\nğŸ§¾ æœ€è¿‘æ˜ç»†ï¼ˆæ–°â†’æ—§ï¼‰")
        for it in items[:20]:
            t = int(it.get("created_at", 0) or 0)
            mod = str(it.get("model_name") or "æœªçŸ¥æ¨¡å‹")
            typ = int(it.get("type", 0) or 0)
            code = int(it.get("code", 0) or 0)
            use = int(it.get("use_time", 0) or 0)
            pt = int(it.get("prompt_tokens", 0) or 0)
            ct = int(it.get("completion_tokens", 0) or 0)
            icon = "ğŸ”´" if typ == 5 or code >= 500 else ("ğŸŸ " if code >= 400 else "ğŸŸ¢")
            lat = "ğŸ¢" if use >= 15000 else ("âš ï¸" if use >= 5000 else "âš¡")
            out.append(f"{icon} {self._fmt_ts(t)} | {mod} | code={code} | {lat}{use}ms | token {pt}/{ct}")

        if len(items) > 20:
            out.append(f"â€¦ å…¶ä½™ {len(items)-20} æ¡å·²çœç•¥ï¼Œå¯ç”¨ /æ—¥å¿— {min(100, len(items))} æŸ¥çœ‹æ›´å¤š")

        return "\n".join(out)

    def _detect_abnormal(self, items: List[Dict[str, Any]]) -> str:
        if not items:
            return "ğŸš¨ å¼‚å¸¸åˆ†æ\næš‚æ— æ—¥å¿—æ•°æ®ï¼Œæ— æ³•åˆ¤æ–­ã€‚"

        m = self._summarize_log_metrics(items)
        errs = m["err_items"]
        total = m["total"]
        err_rate = m["err_rate"]
        slow_rate = m["slow15_rate"]

        lines = ["ğŸš¨ å¼‚å¸¸åˆ†æ"]
        lines.append(
            f"æ€»è¯·æ±‚ {total} | é”™è¯¯ {m['err_count']} ({err_rate:.1%}) | è¶…æ…¢>=15s {m['slow15_count']} ({slow_rate:.1%})"
        )
        lines.append(f"è€—æ—¶åˆ†ä½: p50 {m['p50_ms']}ms | p95 {m['p95_ms']}ms | p99 {m['p99_ms']}ms")

        if err_rate >= 0.2:
            lvl = "P0"
            reason = "é”™è¯¯ç‡è¿‡é«˜ï¼Œå·²æ˜¾è‘—å½±å“å¯ç”¨æ€§"
        elif err_rate >= 0.08 or m["slow15_count"] >= 5:
            lvl = "P1"
            reason = "ç¨³å®šæ€§é€€åŒ–ï¼Œå»ºè®®å°½å¿«å¤„ç†"
        elif err_rate > 0 or m["slow15_count"] > 0:
            lvl = "P2"
            reason = "å­˜åœ¨é›¶æ˜Ÿå¼‚å¸¸ï¼Œå»ºè®®è§‚å¯Ÿå¹¶ä¼˜åŒ–"
        else:
            lvl = "OK"
            reason = "æœªå‘ç°æ˜æ˜¾å¼‚å¸¸"
        lines.append(f"é£é™©ç­‰çº§: {lvl}ï¼ˆ{reason}ï¼‰")

        if m["code_top"]:
            lines.append("çŠ¶æ€ç åˆ†å¸ƒ: " + "ï¼Œ".join([f"{c}({n})" for c, n in m["code_top"]]))
        if m["model_top"]:
            lines.append("é«˜é£é™©æ¨¡å‹å€™é€‰: " + "ï¼Œ".join([f"{x}({n})" for x, n in m["model_top"][:3]]))
        if m["channel_top"]:
            lines.append("é«˜é£é™©æ¸ é“å€™é€‰: " + "ï¼Œ".join([f"{x}({n})" for x, n in m["channel_top"][:3]]))

        if errs:
            lines.append("\nğŸ§¯ æœ€è¿‘é”™è¯¯æ ·æœ¬")
            for it in errs[:8]:
                t = int(it.get("created_at", 0) or 0)
                mod = str(it.get("model_name") or "æœªçŸ¥æ¨¡å‹")
                code = int(it.get("code", 0) or 0)
                use = int(it.get("use_time", 0) or 0)
                chan = str(it.get("channel_name") or it.get("channel") or it.get("channel_id") or "æœªçŸ¥æ¸ é“")
                lines.append(f"- {self._fmt_ts(t)} | {mod} | {chan} | code={code} | {use}ms")

        lines.append("\nâœ… å»ºè®®åŠ¨ä½œ")
        if lvl in ("P0", "P1"):
            lines.append("1) æŒ‰çŠ¶æ€ç å’Œæ¸ é“åšåˆ†ç»„ï¼Œå…ˆåˆ‡æ‰æœ€å·®æ¸ é“éªŒè¯")
            lines.append("2) å¯¹é«˜é£é™©æ¨¡å‹é™å¹¶å‘å¹¶æ”¶æ•› max_tokens")
            lines.append("3) å¯¹ p95>é˜ˆå€¼æ¨¡å‹åšä¸“é¡¹å›æ”¾æ’æŸ¥")
        elif lvl == "P2":
            lines.append("1) ä¼˜å…ˆä¼˜åŒ–æœ€æ…¢ Top5 è¯·æ±‚çš„å‚æ•°ä¸æç¤ºè¯é•¿åº¦")
            lines.append("2) æŒç»­è§‚æµ‹é”™è¯¯ç‡/å»¶è¿Ÿæ›²çº¿ï¼Œé˜²æ­¢æŠ¬å¤´")
        else:
            lines.append("1) å½“å‰å¥åº·ï¼Œå»ºè®®ä¿ç•™åˆ†æ¨¡å‹/åˆ†æ¸ é“å‘¨æŠ¥")

        return "\n".join(lines)

    async def _llm_analyze(self, event: AstrMessageEvent, title: str, content: str) -> str:
        if not self.llm_enabled:
            return "æœªå¼€å¯ LLM åˆ†æï¼ˆè¯·åœ¨é…ç½®ä¸­å¯ç”¨ llm_enabledï¼‰"

        # é»˜è®¤ä½¿ç”¨å½“å‰ä¼šè¯æœåŠ¡å•†ï¼›å¯åˆ‡æ¢ä¸ºæ‰‹åŠ¨æŒ‡å®š provider
        provider_id = ""
        try:
            if self.llm_use_current_provider:
                provider_id = await self.context.get_current_chat_provider_id(umo=event.unified_msg_origin)
            else:
                provider_id = self.llm_provider_id
        except Exception as e:
            return f"è·å–æœåŠ¡å•†å¤±è´¥: {e}"

        if not provider_id:
            return "LLM æœåŠ¡å•†æœªè®¾ç½®ï¼ˆè¯·æ£€æŸ¥ llm_use_current_provider æˆ– llm_provider_idï¼‰"

        prompt = (
            "ä½ æ˜¯èµ„æ·± NewAPI SRE åˆ†æåŠ©æ‰‹ã€‚è¯·ä¸¥æ ¼åŸºäºè¾“å…¥æ•°æ®ï¼Œè¾“å‡ºé¢å‘è¿è¥æ’éšœçš„ä¸­æ–‡æŠ¥å‘Šã€‚\n"
            "ç”¨æˆ·å…³æ³¨ç‚¹ï¼š24h vs 2h æ¶ˆè€—å˜åŒ–ã€æ¨¡å‹ä¸æ¸ é“é›†ä¸­åº¦ã€é”™è¯¯åŸå› ã€æ…¢è¯·æ±‚åŸå› ã€‚\n"
            "ç¦æ­¢æ³›æ³›å»ºè®®ï¼Œä¸è®¨è®ºä½™é¢ä¸é¢„ç®—ï¼Œä¸è¾“å‡ºä¸æ•°æ®æ— å…³å†…å®¹ã€‚\n\n"
            "è¾“å‡ºç»“æ„ï¼ˆå¿…é¡»æŒ‰æ­¤é¡ºåºï¼‰ï¼š\n"
            "# 24h ä¸ 2h æ¶ˆè€—ç»“è®º\n"
            "- 24h ä¸ 2h çš„ token / è¯·æ±‚ / quota å¯¹æ¯”ï¼ˆç»™å‡ºå˜åŒ–æˆ–å æ¯”ï¼‰\n"
            "- è¯´æ˜æœ€è¿‘2å°æ—¶æ˜¯å¦å¼‚å¸¸æ”¾å¤§æˆ–æ”¶ç¼©\n\n"
            "# æ¨¡å‹ä¸æ¸ é“é›†ä¸­åº¦\n"
            "- Top æ¨¡å‹ï¼ˆ24hã€2h å„åˆ—ï¼‰å¹¶è§£é‡Šä¸»è¦è´Ÿè½½æ¨¡å‹\n"
            "- Top æ¸ é“ï¼ˆ24hã€2h å„åˆ—ï¼‰å¹¶è§£é‡Šé›†ä¸­åœ¨å“ªäº›æ¸ é“\n\n"
            "# é”™è¯¯ä¸æ…¢è¯·æ±‚æ ¹å› \n"
            "- æŒ‰çŠ¶æ€ç åˆ†å¸ƒè§£é‡Šä¸»è¦æŠ¥é”™ç±»å‹\n"
            "- ç»“åˆæ¨¡å‹/æ¸ é“/è€—æ—¶æ ·æœ¬ï¼Œç»™å‡ºæœ€å¯èƒ½æ ¹å› ï¼ˆæœ€å¤š3æ¡ï¼‰\n\n"
            "# å¤„ç†å»ºè®®ï¼ˆå¯æ‰§è¡Œï¼‰\n"
            "- ç»™å‡º3~5æ¡æ“ä½œï¼Œä¼˜å…ˆèƒ½ç›´æ¥è½åœ°éªŒè¯\n\n"
            "# å…³é”®ä¿¡å·çœ‹æ¿\n"
            "- åˆ—å‡ºåº”æŒç»­è§‚å¯Ÿçš„æŒ‡æ ‡ï¼šé”™è¯¯ç‡ã€p95ã€p99ã€è¶…æ…¢å æ¯”ã€Topæ¨¡å‹/æ¸ é“å˜åŒ–\n\n"
            "å¦‚æœè¾“å…¥ç¼ºå°‘æ¸ é“æˆ–é”™è¯¯å­—æ®µï¼Œæ˜ç¡®å†™â€œæ•°æ®ç¼ºå¤±ï¼šxxxâ€ï¼Œä¸è¦çŒœæµ‹ã€‚\n\n"
            f"ã€åˆ†æä¸»é¢˜ã€‘{title}\n"
            f"ã€è¾“å…¥æ•°æ®ã€‘\n{content}\n"
        )
        try:
            llm_resp = await self.context.llm_generate(
                chat_provider_id=provider_id,
                prompt=prompt,
            )
            return str(llm_resp.completion_text).strip()
        except Exception as e:
            return f"LLM è°ƒç”¨å¤±è´¥: {e}"

    async def _send_text(self, event: AstrMessageEvent, text: str, use_forward: bool):
        if use_forward:
            try:
                node = Comp.Node(uin=10000, name="newapi", content=[Comp.Plain(text)])
                yield event.chain_result([node])
                return
            except Exception:
                pass
        max_len = 900
        t = text
        while t:
            c = t[:max_len]
            t = t[max_len:]
            yield event.plain_result(c)

    @filter.command("newapi")
    async def cmd_newapi_help(self, event: AstrMessageEvent):
        text = (
            "ğŸ“˜ NewAPI æŒ‡ä»¤\n"
            "/æ¦‚è§ˆ [å°æ—¶]  /æ¨¡å‹ [topN]  /æ—¥å¿— [æ¡æ•°]\n"
            "/é¢åº¦  /å¼‚å¸¸  /åˆ†æ  /å»ºè®®  /å¥åº·\n"
            "\n"
            "ğŸ’¡ LLM æœåŠ¡å•†ï¼š\n"
            "- é»˜è®¤ä½¿ç”¨å½“å‰ä¼šè¯æœåŠ¡å•†ï¼ˆllm_use_current_provider=trueï¼‰\n"
            "- å…³é—­åå¯åœ¨ llm_provider_id ä¸‹æ‹‰æŒ‡å®š"
        )
        async for r in self._send_text(event, text, False):
            yield r

    @filter.command("æ¦‚è§ˆ", alias={"tokensç»Ÿè®¡", "newapiæ¦‚è§ˆ"})
    async def cmd_overview(self, event: AstrMessageEvent, hours: int = 24):
        hours = max(1, min(hours, 168))
        payload = await self._fetch_usage_payload(hours)
        records = self._extract_records(payload)

        # å›é€€æœ¬åœ°ç¼“å­˜
        if not records and self.cache_file.exists():
            try:
                records = self._extract_records(json.loads(self.cache_file.read_text(encoding="utf-8")))
            except Exception:
                pass

        s, e = self._window(hours)
        stats, models = self._aggregate(records, s, e)
        text = self._format_overview(stats, models, self.default_top_n)
        async for r in self._send_text(event, text, self.use_forward):
            yield r

    @filter.command("æ¨¡å‹", alias={"æ¨¡å‹æ’è¡Œ"})
    async def cmd_models(self, event: AstrMessageEvent, topn: int = 5):
        topn = max(1, min(topn, 20))
        payload = await self._fetch_usage_payload(self.default_window_hours)
        records = self._extract_records(payload)
        s, e = self._window(self.default_window_hours)
        stats, models = self._aggregate(records, s, e)
        text = self._format_overview(stats, models, topn)
        async for r in self._send_text(event, text, self.use_forward):
            yield r

    @filter.command("æ—¥å¿—", alias={"logs"})
    async def cmd_logs(self, event: AstrMessageEvent, n: int = 20):
        n = max(1, min(n, 100))
        payload = await self._fetch_logs_payload(n, self.default_window_hours)
        items = self._extract_log_items(payload)
        text = self._format_logs(items)
        async for r in self._send_text(event, text, self.log_use_forward):
            yield r

    @filter.command("é¢åº¦", alias={"æŸ¥è¯¢é¢åº¦"})
    async def cmd_quota(self, event: AstrMessageEvent):
        p = await self._fetch_user_self()
        if isinstance(p, dict) and isinstance(p.get("data"), dict):
            d = p["data"]
            quota = int(d.get("quota", 0) or 0)
            used = int(d.get("used_quota", 0) or 0)
            req = int(d.get("request_count", 0) or 0)
            text = (
                "ğŸ’³ è´¦æˆ·é¢åº¦\n"
                f"ç”¨æˆ·å: {d.get('username', '-') }\n"
                f"åˆ†ç»„: {d.get('group', '-') }\n"
                f"è¯·æ±‚æ¬¡æ•°: {req:,}\n"
                f"å·²ç”¨é…é¢: {used:,}\n"
                f"å½“å‰é¢åº¦(é…é¢/500): $ {quota/500:.2f}"
            )
        else:
            text = f"æŸ¥è¯¢å¤±è´¥: {json.dumps(p, ensure_ascii=False)[:400]}"
        async for r in self._send_text(event, text, self.user_use_forward):
            yield r

    @filter.command("å¼‚å¸¸")
    async def cmd_abnormal(self, event: AstrMessageEvent):
        payload = await self._fetch_logs_payload(max(self.default_log_limit, 30), 24)
        items = self._extract_log_items(payload)
        text = self._detect_abnormal(items)
        async for r in self._send_text(event, text, self.log_use_forward):
            yield r

    @filter.command("åˆ†æ")
    async def cmd_analysis(self, event: AstrMessageEvent):
        # usage: 24h ä¸ 2h åŒçª—å£
        usage_24 = await self._fetch_usage_payload(24)
        usage_2 = await self._fetch_usage_payload(2)
        rec_24 = self._extract_records(usage_24)
        rec_2 = self._extract_records(usage_2)

        s24, e24 = self._window(24)
        s2, e2 = self._window(2)

        stats_24, models_24 = self._aggregate(rec_24, s24, e24)
        stats_2, models_2 = self._aggregate(rec_2, s2, e2)

        channels_24 = self._aggregate_by_keys(
            rec_24,
            s24,
            e24,
            ["channel_name", "channel", "channel_id", "provider_name", "provider", "provider_id"],
            "æœªçŸ¥æ¸ é“",
        )
        channels_2 = self._aggregate_by_keys(
            rec_2,
            s2,
            e2,
            ["channel_name", "channel", "channel_id", "provider_name", "provider", "provider_id"],
            "æœªçŸ¥æ¸ é“",
        )

        # logs: 24h ä¸ 2h åŒçª—å£ï¼ˆç”¨äºé”™è¯¯/è€—æ—¶åˆ†æï¼‰
        logs_24 = await self._fetch_logs_payload(max(self.default_log_limit, 100), 24)
        logs_2 = await self._fetch_logs_payload(max(self.default_log_limit, 100), 2)
        log_items_24 = self._extract_log_items(logs_24)
        log_items_2 = self._extract_log_items(logs_2)

        m24 = self._summarize_log_metrics(log_items_24)
        m2 = self._summarize_log_metrics(log_items_2)

        brief = {
            "dual_window_usage": {
                "24h": {
                    "summary": stats_24,
                    "top_models": [
                        {"name": m, "token": s.get("token", 0), "req": s.get("count", 0), "quota": s.get("quota", 0)}
                        for m, s in models_24[:8]
                    ],
                    "top_channels": [
                        {"name": c, "token": s.get("token", 0), "req": s.get("count", 0), "quota": s.get("quota", 0)}
                        for c, s in channels_24[:8]
                    ],
                },
                "2h": {
                    "summary": stats_2,
                    "top_models": [
                        {"name": m, "token": s.get("token", 0), "req": s.get("count", 0), "quota": s.get("quota", 0)}
                        for m, s in models_2[:8]
                    ],
                    "top_channels": [
                        {"name": c, "token": s.get("token", 0), "req": s.get("count", 0), "quota": s.get("quota", 0)}
                        for c, s in channels_2[:8]
                    ],
                },
            },
            "dual_window_logs": {
                "24h": {
                    "summary": {
                        "total": m24["total"],
                        "err_count": m24["err_count"],
                        "err_rate": round(m24["err_rate"], 4),
                        "slow15_count": m24["slow15_count"],
                        "slow15_rate": round(m24["slow15_rate"], 4),
                        "avg_ms": m24["avg_ms"],
                        "p50_ms": m24["p50_ms"],
                        "p95_ms": m24["p95_ms"],
                        "p99_ms": m24["p99_ms"],
                    },
                    "code_dist": m24["code_top"],
                    "model_dist": m24["model_top"],
                    "channel_dist": m24["channel_top"],
                    "recent_errors": [
                        {
                            "time": self._fmt_ts(int(it.get("created_at", 0) or 0)),
                            "model": str(it.get("model_name") or "æœªçŸ¥æ¨¡å‹"),
                            "channel": str(it.get("channel_name") or it.get("channel") or it.get("channel_id") or "æœªçŸ¥æ¸ é“"),
                            "code": int(it.get("code", 0) or 0),
                            "use_time_ms": int(it.get("use_time", 0) or 0),
                        }
                        for it in m24["err_items"][:12]
                    ],
                    "slow_top": [
                        {
                            "time": self._fmt_ts(int(it.get("created_at", 0) or 0)),
                            "model": str(it.get("model_name") or "æœªçŸ¥æ¨¡å‹"),
                            "channel": str(it.get("channel_name") or it.get("channel") or it.get("channel_id") or "æœªçŸ¥æ¸ é“"),
                            "code": int(it.get("code", 0) or 0),
                            "use_time_ms": int(it.get("use_time", 0) or 0),
                        }
                        for it in m24["slow_items"][:8]
                    ],
                },
                "2h": {
                    "summary": {
                        "total": m2["total"],
                        "err_count": m2["err_count"],
                        "err_rate": round(m2["err_rate"], 4),
                        "slow15_count": m2["slow15_count"],
                        "slow15_rate": round(m2["slow15_rate"], 4),
                        "avg_ms": m2["avg_ms"],
                        "p50_ms": m2["p50_ms"],
                        "p95_ms": m2["p95_ms"],
                        "p99_ms": m2["p99_ms"],
                    },
                    "code_dist": m2["code_top"],
                    "model_dist": m2["model_top"],
                    "channel_dist": m2["channel_top"],
                    "recent_errors": [
                        {
                            "time": self._fmt_ts(int(it.get("created_at", 0) or 0)),
                            "model": str(it.get("model_name") or "æœªçŸ¥æ¨¡å‹"),
                            "channel": str(it.get("channel_name") or it.get("channel") or it.get("channel_id") or "æœªçŸ¥æ¸ é“"),
                            "code": int(it.get("code", 0) or 0),
                            "use_time_ms": int(it.get("use_time", 0) or 0),
                        }
                        for it in m2["err_items"][:12]
                    ],
                    "slow_top": [
                        {
                            "time": self._fmt_ts(int(it.get("created_at", 0) or 0)),
                            "model": str(it.get("model_name") or "æœªçŸ¥æ¨¡å‹"),
                            "channel": str(it.get("channel_name") or it.get("channel") or it.get("channel_id") or "æœªçŸ¥æ¸ é“"),
                            "code": int(it.get("code", 0) or 0),
                            "use_time_ms": int(it.get("use_time", 0) or 0),
                        }
                        for it in m2["slow_items"][:8]
                    ],
                },
            },
        }

        preface = self._format_dual_window_report(
            stats_24, models_24, channels_24,
            stats_2, models_2, channels_2,
            m24["channel_top"], m2["channel_top"],
        )
        llm_text = await self._llm_analyze(event, "24h/2h æ¶ˆè€—ä¸æ—¥å¿—åˆ†æ", json.dumps(brief, ensure_ascii=False))
        text = preface + "\n\n" + llm_text
        async for r in self._send_text(event, text, self.use_forward):
            yield r

    @filter.command("å»ºè®®")
    async def cmd_advice(self, event: AstrMessageEvent):
        logs = await self._fetch_logs_payload(max(self.default_log_limit, 50), 24)
        log_items = self._extract_log_items(logs)
        raw = self._detect_abnormal(log_items)
        text = await self._llm_analyze(event, "ä¼˜åŒ–å»ºè®®", raw)
        async for r in self._send_text(event, text, self.use_forward):
            yield r

    @filter.command("å¥åº·", alias={"health"})
    async def cmd_health(self, event: AstrMessageEvent):
        out = ["ğŸ©º å¥åº·æ£€æŸ¥"]
        out.append(f"plugin_version: 2.3.1")
        out.append(f"base_domain: {'OK' if self.base_domain else 'ç¼ºå¤±'}")
        out.append(f"authorization: {'OK' if self.authorization else 'ç¼ºå¤±'}")
        out.append(f"new_api_user: {'OK' if self.new_api_user else 'ç¼ºå¤±'}")

        if self.base_domain:
            p1 = await self._fetch_user_self()
            ok1 = isinstance(p1, dict) and not p1.get('error') and p1.get('success', True)
            out.append(f"/api/user/self: {'OK' if ok1 else 'FAIL'}")
            p2 = await self._fetch_logs_payload(1, 1)
            ok2 = isinstance(p2, dict) and not p2.get('error') and p2.get('success', True)
            out.append(f"/api/log/: {'OK' if ok2 else 'FAIL'}")
            p3 = await self._fetch_usage_payload(1)
            ok3 = isinstance(p3, dict) and not p3.get('error') and p3.get('success', True)
            out.append(f"/api/data/self: {'OK' if ok3 else 'FAIL'}")

        if self.llm_enabled:
            if self.llm_use_current_provider:
                out.append("LLM: å·²å¯ç”¨ï¼ˆä½¿ç”¨å½“å‰ä¼šè¯æœåŠ¡å•†ï¼‰")
            else:
                out.append(f"LLM: å·²å¯ç”¨ï¼ˆæ‰‹åŠ¨æœåŠ¡å•†: {self.llm_provider_id or 'æœªè®¾ç½®'}ï¼‰")
        else:
            out.append("LLM: æœªå¯ç”¨")

        async for r in self._send_text(event, "\n".join(out), False):
            yield r

    async def terminate(self):
        logger.info("newapi æ’ä»¶å·²å¸è½½")
